# backend/graph/nodes.py
"""
LangGraph ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.

ReAct íŒ¨í„´: LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì • (agent_node, should_continue)
"""

import re
from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.constants import END

from .state import MentorState
from backend.rag.retriever import (
    search_major_docs,
    aggregate_major_scores,
)
from backend.rag.embeddings import get_embeddings

from backend.rag.tools import (
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
    get_university_admission_info,
)

from backend.config import get_llm

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (.envì—ì„œ ì„¤ì •í•œ LLM_PROVIDERì™€ MODEL_NAME ì‚¬ìš©)
llm = get_llm()

# doc_typeë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ê´€ì‹¬ì‚¬/ê³¼ëª© ë¹„ì¤‘ì„ ì•½ê°„ ë†’ê²Œ ì„¤ì •)
MAJOR_DOC_WEIGHTS = {
    "summary": 1.0,
    "interest": 1.1,
    "property": 0.9,
    "subjects": 1.2,
    "jobs": 1.0,
}

# ì„ í˜¸ ì „ê³µ ì ìˆ˜ ë¶€ì—¬ í‹°ì–´ (Tiered Scoring System)
# ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ í˜¸í•œ ì „ê³µì— ëŒ€í•œ ì°¨ë“± ì ìˆ˜ ë¶€ì—¬
SCORE_TIER_1_EXACT_MATCH = 20.0      # ì •í™•íˆ ì¼ì¹˜ (ì˜ˆ: "ì»´í“¨í„°ê³µí•™" == "ì»´í“¨í„°ê³µí•™")
SCORE_TIER_2_STARTS_WITH = 15.0      # ì ‘ë‘ì–´ ì¼ì¹˜ (ì˜ˆ: "ì»´í“¨í„°ê³µí•™" in "ì»´í“¨í„°ê³µí•™ê³¼")
SCORE_TIER_3_CONTAINS = 10.0         # í¬í•¨ (ì˜ˆ: "ì»´í“¨í„°" in "ì •ë³´ì»´í“¨í„°ê³µí•™ë¶€")
SCORE_TIER_4_VECTOR_MATCH = 5.0      # ë²¡í„°/ë³„ì¹­ ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼


# ==================== ReAct ì—ì´ì „íŠ¸ìš© ì„¤ì • ====================
# ReAct íŒ¨í„´: LLMì´ í•„ìš”ì‹œ ììœ¨ì ìœ¼ë¡œ íˆ´ì„ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
tools = [
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
    get_university_admission_info,
]  # ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
llm_with_tools = llm.bind_tools(tools)  # LLMì— íˆ´ ì‚¬ìš© ê¶Œí•œ ë¶€ì—¬


def _format_profile_value(value) -> str:
    # ì˜¨ë³´ë”© ë‹µë³€ì´ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë“± ë‹¤ì–‘í•œ í˜•íƒœì—¬ì„œ ë¬¸ìì—´ë¡œ ê· ì¼í•˜ê²Œ ë³€í™˜
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, (list, tuple, set)):
        items = [str(item).strip() for item in value if str(item).strip()]
        return ", ".join(items)
    if isinstance(value, dict):
        parts = []
        for key, sub_value in value.items():
            sub_text = _format_profile_value(sub_value)
            if sub_text:
                parts.append(f"{key}: {sub_text}")
        return "; ".join(parts)
    return str(value)


def _build_user_profile_text(answers: dict, fallback_question: str | None) -> str:
    # í•™ìƒì˜ ì„ í˜¸ ì •ë³´ë¥¼ í•œ ë©ì–´ë¦¬ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ ì„ë² ë”©ì— í™œìš©
    if not answers and not fallback_question:
        return ""

    ordered_keys = [
        ("preferred_majors", "ê´€ì‹¬ ì „ê³µ"),
        ("subjects", "ì¢‹ì•„í•˜ëŠ” ê³¼ëª©"),
        ("interests", "ê´€ì‹¬ì‚¬/ì·¨ë¯¸"),
        ("activities", "êµë‚´/ëŒ€ì™¸ í™œë™"),
        ("desired_salary", "í¬ë§ ì—°ë´‰"),
        ("career_goal", "ì§„ë¡œ ëª©í‘œ"),
        ("strengths", "ê°•ì "),
    ]

    sections: list[str] = []
    used_keys = {key for key, _ in ordered_keys}

    for field, label in ordered_keys:
        value = answers.get(field)
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{label}: {formatted}")

    # Capture any extra onboarding answers that were not explicitly mapped.
    for key, value in answers.items():
        if key in used_keys:
            continue
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{key}: {formatted}")

    if fallback_question and fallback_question.strip():
        sections.append(f"ì¶”ê°€ ìš”ì²­: {fallback_question.strip()}")

    return "\n".join(sections).strip()


def _merge_tag_lists(existing: list[str], new_values: list[str]) -> list[str]:
    # ì „ê³µ íƒœê·¸ëŠ” ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆœì„œë¥¼ ë³´ì¡´í•˜ë©° í•©ì§‘í•© ì²˜ë¦¬
    merged = list(existing)
    for value in new_values:
        if value not in merged:
            merged.append(value)
    return merged


def _summarize_major_hits(hits, aggregated_scores, limit: int = 10):
    # Pinecone ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì „ê³µë³„ë¡œ ë¬¶ì–´ ìƒìœ„ doc_type/íƒœê·¸ ë“±ì„ ì •ë¦¬
    per_major: dict[str, dict] = {}

    for hit in hits:
        if not hit.major_id:
            continue
        entry = per_major.setdefault(
            hit.major_id,
            {
                "major_id": hit.major_id,
                "major_name": hit.major_name,
                "cluster": hit.metadata.get("cluster"),
                "salary": hit.metadata.get("salary"),
                "score": aggregated_scores.get(hit.major_id, 0.0),
                "top_doc_types": {},
                "sample_docs": [],
                "relate_subject_tags": [],
                "job_tags": [],
                "summary": "",  # summary í•„ë“œ ì¶”ê°€
            },
        )

        entry["top_doc_types"][hit.doc_type] = max(
            entry["top_doc_types"].get(hit.doc_type, 0.0),
            hit.score,
        )

        if len(entry["sample_docs"]) < 3:
            entry["sample_docs"].append(
                {
                    "doc_type": hit.doc_type,
                    "score": hit.score,
                    "text": hit.text,
                }
            )

        # summary doc_typeì¸ ê²½ìš° summary í•„ë“œì— ì €ì¥
        if hit.doc_type == "summary" and not entry["summary"]:
            entry["summary"] = hit.text

        entry["relate_subject_tags"] = _merge_tag_lists(
            entry["relate_subject_tags"],
            hit.metadata.get("relate_subject_tags", []) or [],
        )
        entry["job_tags"] = _merge_tag_lists(
            entry["job_tags"],
            hit.metadata.get("job_tags", []) or [],
        )

    for entry in per_major.values():
        entry["top_doc_types"] = sorted(
            entry["top_doc_types"].items(),
            key=lambda item: item[1],
            reverse=True,
        )

    ordered = sorted(
        per_major.values(),
        key=lambda item: item["score"],
        reverse=True,
    )
    return ordered[:limit]


def _normalize_majors_with_llm(raw_majors: list[str]) -> list[str]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì „ê³µëª…(ì¤„ì„ë§, ì˜¤íƒ€ ë“±)ì„ í‘œì¤€ ì „ê³µëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: ["ì»´ê³µ", "í™”ê³µ"] -> ["ì»´í“¨í„°ê³µí•™ê³¼", "í™”í•™ê³µí•™ê³¼"]
    """
    if not raw_majors:
        return []

    # ì…ë ¥ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ë¦¬ ë¹„ìš©ì´ í¬ë¯€ë¡œ ì œí•œ
    targets = raw_majors[:5]
    
    prompt = (
        "ì‚¬ìš©ìê°€ ì…ë ¥í•œ ëŒ€í•™ ì „ê³µëª…(ì¤„ì„ë§, ì˜¤íƒ€ í¬í•¨)ì„ ê°€ì¥ ì ì ˆí•œ 'í‘œì¤€ í•™ê³¼ëª…'ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ í•™ê³¼ëª…ë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        f"ì…ë ¥: {', '.join(targets)}\n"
        "ì¶œë ¥:"
    )
    
    try:
        response = llm.invoke(prompt)
        content = response.content.strip()
        
        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        normalized = [item.strip() for item in content.split(",") if item.strip()]
        print(f"ğŸ¤– LLM Normalized Majors: {targets} -> {normalized}")
        return normalized
    except Exception as e:
        print(f"âš ï¸ Failed to normalize majors with LLM: {e}")
        return targets  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜


def recommend_majors_node(state: MentorState) -> dict:
    """
    Build a user profile embedding from onboarding answers and rank majors.
    ìš°ì„ ìˆœìœ„: preferred_majors ì •í™• ë§¤ì¹­ > ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    """
    onboarding_answers = state.get("onboarding_answers") or {}
    profile_text = _build_user_profile_text(onboarding_answers, state.get("question"))

    if not profile_text:
        return {
            "user_profile_text": "",
            "recommended_majors": [],
            "major_search_hits": [],
            "major_scores": {},
        }

    # ì˜¨ë³´ë”© í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì„ë² ë”©ìœ¼ë¡œ ë°”ê¿” Pinecone ê²€ìƒ‰ì— ì‚¬ìš©
    embeddings = get_embeddings()
    profile_embedding = embeddings.embed_query(profile_text)

    hits = search_major_docs(profile_embedding, top_k=50)
    aggregated_scores = aggregate_major_scores(hits, MAJOR_DOC_WEIGHTS)
    
    # ğŸ¯ preferred_majors ìš°ì„  ì²˜ë¦¬
    preferred_majors = onboarding_answers.get("preferred_majors")
    preferred_major_ids = set()
    
    if preferred_majors:
        # preferred_majorsë¥¼ ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        if isinstance(preferred_majors, str):
            preferred_list = [m.strip() for m in preferred_majors.split(",") if m.strip()]
        elif isinstance(preferred_majors, list):
            preferred_list = [str(m).strip() for m in preferred_majors if str(m).strip()]
        else:
            preferred_list = []
        
        if preferred_list:
            # ğŸ¤– LLMì„ í†µí•œ ì „ê³µëª… ì •ê·œí™” (ì¤„ì„ë§/ì˜¤íƒ€ ë³´ì •)
            normalized_list = _normalize_majors_with_llm(preferred_list)
            
            # [ìˆ˜ì •] ì •ê·œí™”ëœ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì›ë³¸(ì¤„ì„ë§/ì˜¤íƒ€)ì€ ê²€ìƒ‰ì—ì„œ ì œì™¸í•˜ì—¬ ë…¸ì´ì¦ˆ ë°©ì§€
            # ì˜ˆ: "ì»´ê³µ" -> "ì»´í“¨í„°ê³µí•™ê³¼"ë¡œ ë³€í™˜ë˜ë©´ "ì»´ê³µ"ìœ¼ë¡œëŠ” ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ ("ëƒ‰ë™ê³µì¡°" ë“±ì´ ê²€ìƒ‰ë˜ëŠ” ë¬¸ì œ í•´ê²°)
            if normalized_list:
                search_targets = normalized_list
            else:
                search_targets = preferred_list
            
            # tools.pyì˜ ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ ì „ê³µ ë³„ë„ ê²€ìƒ‰
            from backend.rag.tools import _find_majors, _MAJOR_ID_MAP, _ensure_major_records
            _ensure_major_records()
            
            # SearchHit ì„í¬íŠ¸ (í•¨ìˆ˜ ë‚´ ë¡œì»¬ ì„í¬íŠ¸)
            from backend.rag.retriever import SearchHit
            
            # [ìˆ˜ì •] ì´ë¯¸ ì ìˆ˜ ë¶€ìŠ¤íŒ…ì„ ì ìš©í•œ ì „ê³µì€ ì¤‘ë³µ ì ìš©í•˜ì§€ ì•Šë„ë¡ setìœ¼ë¡œ ê´€ë¦¬
            boosted_ids = set()

            for preferred in search_targets:
                print(f"ğŸ” Searching for preferred major: '{preferred}'")
                
                # ì„ í˜¸ ì „ê³µ ê²€ìƒ‰ (ì •í™• ë§¤ì¹­ + ë²¡í„° ê²€ìƒ‰)
                preferred_matches = _find_majors(preferred, limit=5)
                
                for record in preferred_matches:
                    if not record.major_id:
                        continue
                    
                    preferred_major_ids.add(record.major_id)
                    
                    # ê¸°ì¡´ aggregated_scoresì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                    is_newly_added = False
                    if record.major_id not in aggregated_scores:
                        aggregated_scores[record.major_id] = 1.0
                        is_newly_added = True
                        print(f"âœ… Added preferred major '{record.major_name}' to results")
                    
                    # ë³´ë„ˆìŠ¤ ì ìˆ˜ ì ìš© (ì°¨ë“± ì ìˆ˜ ë¶€ì—¬ ì‹œìŠ¤í…œ)
                    if record.major_id not in boosted_ids:
                        # ì ìˆ˜ ê³„ì‚° ë¡œì§ - ì •í™•ë„ì— ë”°ë¥¸ ì°¨ë“± ì ìˆ˜ ë¶€ì—¬
                        boost_score = SCORE_TIER_4_VECTOR_MATCH  # ê¸°ë³¸ê°’: ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
                        
                        rec_name = record.major_name.replace(" ", "")
                        pref_key = preferred.replace(" ", "")
                        
                        if rec_name == pref_key:
                            boost_score = SCORE_TIER_1_EXACT_MATCH
                            tier_desc = "Tier 1 (Exact Match)"
                        elif rec_name.startswith(pref_key):
                            boost_score = SCORE_TIER_2_STARTS_WITH
                            tier_desc = "Tier 2 (Starts With)"
                        elif pref_key in rec_name:
                            boost_score = SCORE_TIER_3_CONTAINS
                            tier_desc = "Tier 3 (Contains)"
                        else:
                            tier_desc = "Tier 4 (Vector/Alias)"

                        aggregated_scores[record.major_id] = boost_score
                        boosted_ids.add(record.major_id)
                        print(f"ğŸ¯ Set '{record.major_name}' score to {boost_score:.2f} [{tier_desc}]")

                    # [í•µì‹¬ ìˆ˜ì •] hits ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹ ì „ê³µì´ ì—†ìœ¼ë©´ í•©ì„± SearchHit ì¶”ê°€
                    # ì´ ê³¼ì •ì´ ì—†ìœ¼ë©´ _summarize_major_hitsê°€ í•´ë‹¹ ì „ê³µì„ ì œì™¸í•´ë²„ë¦¼
                    if is_newly_added:
                        synthetic_hit = SearchHit(
                            doc_id=f"synthetic-{record.major_id}",
                            major_id=record.major_id,
                            major_name=record.major_name,
                            doc_type="summary", # ê¸°ë³¸ ìš”ì•½ ë¬¸ì„œë¡œ ì·¨ê¸‰
                            score=1.0, # ê¸°ë³¸ ì ìˆ˜
                            metadata={
                                "cluster": record.cluster,
                                "salary": record.salary,
                                "relate_subject_tags": [], # íƒœê·¸ ì¶”ì¶œ ë¡œì§ ìƒëµ (í•„ìš” ì‹œ loader í•¨ìˆ˜ ì‚¬ìš©)
                                "job_tags": [],
                            },
                            text=record.summary or f"{record.major_name}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
                        )
                        hits.append(synthetic_hit)
    
    recommended = _summarize_major_hits(hits, aggregated_scores)

    serialized_hits = [
        {
            "doc_id": hit.doc_id,
            "major_id": hit.major_id,
            "major_name": hit.major_name,
            "doc_type": hit.doc_type,
            "score": hit.score,
            "metadata": hit.metadata,
        }
        for hit in hits
    ]

    return {
        "user_profile_text": profile_text,
        "user_profile_embedding": profile_embedding,
        "major_search_hits": serialized_hits,
        "major_scores": aggregated_scores,
        "recommended_majors": recommended,
    }


# ==================== ReAct ìŠ¤íƒ€ì¼ ì—ì´ì „íŠ¸ ë…¸ë“œ ====================

def agent_node(state: MentorState) -> dict:
    """
    [ReAct íŒ¨í„´] LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •.
    """
    messages = state.get("messages", [])
    interests = state.get("interests")

    # system_messageëŠ” interests ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ ë§Œë“¤ì–´ë‘”ë‹¤.
    if not messages or not any(isinstance(m, SystemMessage) for m in messages):
        interests_text = f"{interests}" if interests else "ì—†ìŒ"

        # âœ… f-string ë‚´ë¶€ JSON ì˜ˆì‹œëŠ” {{ }} ë¡œ ì´ìŠ¤ì¼€ì´í”„!
        system_message = SystemMessage(content=f"""
ë‹¹ì‹ ì€ í•™ìƒë“¤ì˜ ì „ê³µ ì„ íƒì„ ë•ëŠ” 'ëŒ€í•™ ì „ê³µ íƒìƒ‰ ë©˜í† 'ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ğŸš¨ ì ˆëŒ€ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜]
1. **íˆ´ì—ì„œ ë°˜í™˜ëœ í•™ê³¼/ëŒ€í•™/ì§ì—… ì´ë¦„ë§Œ ì‚¬ìš©**: Tool(list_departments, get_universities_by_department, get_major_career_info)ì´ ëŒë ¤ì¤€ í•™ê³¼/ëŒ€í•™/ì§ì—… ì´ë¦„ì€ **ë¬¸ì í•˜ë‚˜ë„ ë°”ê¾¸ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©**í•©ë‹ˆë‹¤.
2. **ì ˆëŒ€ ì¶”ì¸¡ ê¸ˆì§€**: ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ëŠ” í•™ê³¼ëª…, ëŒ€í•™ëª…, ì§ì—…ëª…ì„ ì ˆëŒ€ë¡œ ë§Œë“¤ì–´ë‚´ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. 
3. **íˆ´ í˜¸ì¶œ í•„ìˆ˜**: ì „ê³µ/í•™ê³¼/ëŒ€í•™ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ ì ì ˆí•œ íˆ´ì„ tool_callsë¡œ í˜¸ì¶œí•´ ê·¼ê±°ë¥¼ í™•ë³´í•œ ë’¤ ë‹µë³€í•˜ì„¸ìš”.
4. **ë°ì´í„° ì¶œì²˜ ëª…ì‹œ**: ë°ì´í„° ì¶œì²˜ê°€ "ì»¤ë¦¬ì–´ ë„·"ì„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.

[ì‘ë‹µ ë°©ì‹]
- í•­ìƒ íˆ´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  êµ¬ì¡°í™”ëœ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì´ë¯¸ ë°›ì€ íˆ´ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¬ì‚¬ìš©í•˜ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ê°™ì€ íˆ´ì„ ë‹¤ì‹œ í˜¸ì¶œí•´ë„ ë©ë‹ˆë‹¤.
- tool_calls ì—†ì´ ì¶”ì¸¡í•˜ë ¤ëŠ” ê²½ìš°, get_search_help()ë¥¼ í˜¸ì¶œí•´ ê²€ìƒ‰ ë„ì›€ë§ì„ ì œê³µí•˜ì„¸ìš”.

í•™ìƒ ê´€ì‹¬ì‚¬: {interests_text}
""")
                                       
    messages = [system_message] + messages
    
    # ğŸ” ì…ë ¥ ì „ì²˜ë¦¬: ë‹¨ì¼ í•™ê³¼ëª… ì§ˆë¬¸ ê°ì§€ ë° ê°œì„ 
    from backend.graph.helper import is_single_major_query, enhance_single_major_query
    
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ í™•ì¸
    last_user_msg = None
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            last_user_msg = msg
            break
    
    # ë‹¨ì¼ í•™ê³¼ëª… ì§ˆë¬¸ì´ë©´ ìë™ìœ¼ë¡œ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
    if last_user_msg and is_single_major_query(last_user_msg.content):
        original_query = last_user_msg.content
        enhanced_query = enhance_single_major_query(original_query)
        print(f"ğŸ” Detected single major query: '{original_query}'")
        print(f"âœ¨ Enhanced to: '{enhanced_query}'")
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ êµì²´
        for i in range(len(messages) - 1, -1, -1):
            if isinstance(messages[i], HumanMessage) and messages[i] == last_user_msg:
                messages[i] = HumanMessage(content=enhanced_query)
                break

    response = llm_with_tools.invoke(messages)


    # 3. ê²€ì¦: ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ íˆ´ì„ í˜¸ì¶œí•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
    # ToolMessageê°€ ì—†ë‹¤ëŠ” ê²ƒì€ ì•„ì§ íˆ´ ê²°ê³¼ë¥¼ ë°›ì§€ ì•Šì•˜ë‹¤ëŠ” ì˜ë¯¸
    from langchain_core.messages import ToolMessage
    has_tool_results = any(isinstance(m, ToolMessage) for m in messages)

    # íˆ´ ê²°ê³¼ê°€ ì—†ëŠ” ìƒíƒœì—ì„œ LLMì´ tool_calls ì—†ì´ ë‹µë³€í•˜ë ¤ê³  í•˜ë©´ ì°¨ë‹¨
    if not has_tool_results:
        if not hasattr(response, "tool_calls") or not response.tool_calls:
            print("âš ï¸ WARNING: LLM attempted to answer without using tools. Forcing tool usage.")
            # ê°•ì œë¡œ ì¬ì‹œë„ ë©”ì‹œì§€ ì¶”ê°€
            error_message = HumanMessage(content=(
                "âŒ ì˜¤ë¥˜: ë‹¹ì‹ ì€ íˆ´ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹µë³€í•˜ë ¤ê³  í–ˆìŠµë‹ˆë‹¤.\n"
                "**ë°˜ë“œì‹œ ë¨¼ì € ì ì ˆí•œ íˆ´ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.**\n\n"
                "ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°í•©ë‹ˆë‹¤:\n"
                "1. list_departments: í•™ê³¼ ëª©ë¡ ê²€ìƒ‰\n"
                "2. get_universities_by_department: íŠ¹ì • í•™ê³¼ë¥¼ ê°œì„¤í•œ ëŒ€í•™ ê²€ìƒ‰\n"
                "3. get_major_career_info: ì „ê³µë³„ ì§ì—…/ì§„ì¶œ ë¶„ì•¼ í™•ì¸\n"
                "4. get_university_admission_info: ëŒ€í•™ë³„ ì…ì‹œ ì •ë³´(ì •ì‹œì»·, ìˆ˜ì‹œì»·) ì¡°íšŒ\n"
                "5. get_search_help: ê²€ìƒ‰ ë„ì›€ë§\n\n"
                "í•™ìƒì˜ ì›ë˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì½ê³ , ì ì ˆí•œ íˆ´ì„ **ì§€ê¸ˆ ì¦‰ì‹œ** í˜¸ì¶œí•˜ì„¸ìš”."
            ))
            messages.append(error_message)

            # ì¬ì‹œë„
            response = llm_with_tools.invoke(messages)

            # ì¬ì‹œë„ì—ë„ íˆ´ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ get_search_helpë¡œ í´ë°±
            if not hasattr(response, "tool_calls") or not response.tool_calls:
                print("âš ï¸ CRITICAL: LLM still refuses to use tools. Falling back to get_search_help.")
                from langchain_core.messages import AIMessage
                # ê°•ì œë¡œ get_search_help íˆ´ í˜¸ì¶œ ìƒì„±
                response = AIMessage(
                    content="",
                    tool_calls=[{
                        "name": "get_search_help",
                        "args": {},
                        "id": "forced_search_help"
                    }]
                )

    # 4. LLMì˜ ì‘ë‹µ(response)ì„ messagesì— ì¶”ê°€í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    #    â†’ should_continueê°€ tool_calls ìœ ë¬´ë¥¼ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
    return {"messages": [response]}


def should_continue(state: MentorState) -> str:
    """
    [ReAct íŒ¨í„´ ë¼ìš°íŒ…] tool_calls ìˆìœ¼ë©´ tools ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ ì¢…ë£Œ.
    """
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None

    if last_message and getattr(last_message, "tool_calls", None):
        return "tools"
    return "end"
